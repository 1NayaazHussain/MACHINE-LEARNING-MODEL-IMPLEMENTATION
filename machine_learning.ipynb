import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.pipeline import Pipeline
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import joblib
import warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

print("ðŸš€ Spam Email Detection Model Implementation")
print("=" * 50)

# Create sample dataset (in real scenario, you'd load from CSV)
# This simulates the SMS Spam Collection Dataset
sample_data = {
    'message': [
        "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
        "Ok lar... Joking wif u oni...",
        "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
        "U dun say so early hor... U c already then say...",
        "Nah I don't think he goes to usf, he lives around here though",
        "WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.",
        "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030",
        "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.",
        "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info",
        "URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18",
        "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.",
        "I HAVE A DATE ON SUNDAY WITH WILL!!",
        "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap.xxxmobilemovieclub.com?n=QJKGIGHJJGCBL",
        "Oh k...i'm watching here:)",
        "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.",
        "Fine if thats the way u feel. Thats the way its gota b"
    ],
    'label': ['ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham']
}

# Create DataFrame
df = pd.DataFrame(sample_data)

# Expand dataset by duplicating and adding variations
expanded_data = []
labels = []

# Add original data
for msg, label in zip(df['message'], df['label']):
    expanded_data.append(msg)
    labels.append(label)

# Add more spam examples
spam_examples = [
    "CONGRATULATIONS! You've won Â£1000 cash! Call now 09061743386",
    "FREE ringtones! Text TONE to 85080",
    "URGENT! Your mobile number has won Â£5000! Claim now!",
    "Win a Â£1000 cash prize! Text WIN to 80082",
    "FINAL NOTICE: You have 24hrs to claim your prize",
    "Click here for FREE mobile content",
    "You have been selected for a special offer! Act now!",
    "WINNER! Congratulations you have won a cash prize",
    "FREE entry to win Â£100 weekly! Text PLAY to 83049",
    "Your number has been selected! Call 09061743386 now!"
]

# Add more ham examples
ham_examples = [
    "Hey, are we still meeting for lunch tomorrow?",
    "Thanks for the birthday wishes!",
    "Can you pick up some milk on your way home?",
    "The meeting has been moved to 3pm",
    "Hope you're having a great day!",
    "Don't forget about dinner tonight",
    "How was your weekend?",
    "See you at the office tomorrow",
    "Thanks for your help with the project",
    "Let me know when you're free to chat"
]

# Add expanded examples
for msg in spam_examples:
    expanded_data.append(msg)
    labels.append('spam')

for msg in ham_examples:
    expanded_data.append(msg)
    labels.append('ham')

# Create expanded DataFrame
df = pd.DataFrame({'message': expanded_data, 'label': labels})

print(f"ðŸ“Š Dataset Overview:")
print(f"Total messages: {len(df)}")
print(f"Spam messages: {len(df[df['label'] == 'spam'])}")
print(f"Ham messages: {len(df[df['label'] == 'ham'])}")
print("\nLabel distribution:")
print(df['label'].value_counts())
print()

# Text preprocessing function
def preprocess_text(text):
    """
    Preprocess text data for machine learning
    """
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove digits
    text = re.sub(r'\d+', '', text)
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    text = ' '.join([word for word in words if word not in stop_words])
    
    # Stemming
    stemmer = PorterStemmer()
    words = text.split()
    text = ' '.join([stemmer.stem(word) for word in words])
    
    return text

print("ðŸ”„ Preprocessing text data...")
df['processed_message'] = df['message'].apply(preprocess_text)

# Prepare features and target
X = df['processed_message']
y = df['label']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set size: {len(X_train)}")
print(f"Test set size: {len(X_test)}")
print()

# Feature extraction using TF-IDF
print("ðŸ” Extracting features using TF-IDF...")
tfidf = TfidfVectorizer(max_features=3000, min_df=2, max_df=0.8)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

print(f"Feature matrix shape: {X_train_tfidf.shape}")
print()

# Initialize models
models = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(random_state=42),
    'SVM': SVC(random_state=42, probability=True),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate models
results = {}
print("ðŸ¤– Training and evaluating models...")
print("-" * 40)

for name, model in models.items():
    print(f"Training {name}...")
    
    # Train the model
    model.fit(X_train_tfidf, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test_tfidf)
    y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test == 'spam', y_pred_proba)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5)
    
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'roc_auc': roc_auc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  ROC-AUC: {roc_auc:.4f}")
    print(f"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print()

# Find best model
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']

print(f"ðŸ† Best Model: {best_model_name}")
print(f"Best Accuracy: {results[best_model_name]['accuracy']:.4f}")
print()

# Detailed evaluation of best model
print(f"ðŸ“ˆ Detailed Evaluation - {best_model_name}")
print("-" * 40)

y_pred_best = results[best_model_name]['y_pred']
print("Classification Report:")
print(classification_report(y_test, y_pred_best))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_best)
print("Confusion Matrix:")
print(cm)
print()

# Feature importance (for models that support it)
if hasattr(best_model, 'feature_importances_'):
    feature_names = tfidf.get_feature_names_out()
    feature_importance = best_model.feature_importances_
    top_features = sorted(zip(feature_names, feature_importance), 
                         key=lambda x: x[1], reverse=True)[:10]
    
    print("Top 10 Most Important Features:")
    for feature, importance in top_features:
        print(f"  {feature}: {importance:.4f}")
    print()

# Model comparison visualization data
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
roc_aucs = [results[name]['roc_auc'] for name in model_names]

print("ðŸ“Š Model Comparison:")
for i, name in enumerate(model_names):
    print(f"{name:20} | Accuracy: {accuracies[i]:.4f} | ROC-AUC: {roc_aucs[i]:.4f}")
print()

# Hyperparameter tuning for best model
print(f"ðŸ”§ Hyperparameter Tuning for {best_model_name}...")

if best_model_name == 'Naive Bayes':
    param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}
elif best_model_name == 'Logistic Regression':
    param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}
elif best_model_name == 'SVM':
    param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
else:  # Random Forest
    param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}

grid_search = GridSearchCV(
    models[best_model_name], 
    param_grid, 
    cv=3, 
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train_tfidf, y_train)
tuned_model = grid_search.best_estimator_

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# Evaluate tuned model
y_pred_tuned = tuned_model.predict(X_test_tfidf)
tuned_accuracy = accuracy_score(y_test, y_pred_tuned)
print(f"Tuned model accuracy: {tuned_accuracy:.4f}")
print()

# Save the best model
print("ðŸ’¾ Saving the best model...")
model_filename = 'spam_detection_model.joblib'
vectorizer_filename = 'tfidf_vectorizer.joblib'

joblib.dump(tuned_model, model_filename)
joblib.dump(tfidf, vectorizer_filename)

print(f"Model saved as: {model_filename}")
print(f"Vectorizer saved as: {vectorizer_filename}")
print()

# Prediction function
def predict_spam(message, model=tuned_model, vectorizer=tfidf):
    """
    Predict if a message is spam or ham
    """
    processed_message = preprocess_text(message)
    message_tfidf = vectorizer.transform([processed_message])
    prediction = model.predict(message_tfidf)[0]
    probability = model.predict_proba(message_tfidf)[0]
    
    return {
        'prediction': prediction,
        'spam_probability': probability[1] if hasattr(probability, '__len__') and len(probability) > 1 else probability[0],
        'ham_probability': probability[0] if hasattr(probability, '__len__') and len(probability) > 1 else 1 - probability[0]
    }

# Test the prediction function
print("ðŸ§ª Testing the model with sample messages:")
test_messages = [
    "Congratulations! You've won a Â£1000 prize! Call now!",
    "Hey, are we still meeting for lunch tomorrow?",
    "FREE ringtones! Text TONE to 85080",
    "Thanks for your help with the project"
]

for msg in test_messages:
    result = predict_spam(msg)
    print(f"Message: '{msg[:50]}...'")
    print(f"Prediction: {result['prediction']}")
    print(f"Spam probability: {result['spam_probability']:.4f}")
    print("-" * 30)

print("âœ… Spam Detection Model Implementation Complete!")
print("\nModel Performance Summary:")
print(f"- Best Model: {best_model_name}")
print(f"- Final Accuracy: {tuned_accuracy:.4f}")
print(f"- ROC-AUC Score: {results[best_model_name]['roc_auc']:.4f}")
print(f"- Cross-validation Score: {results[best_model_name]['cv_mean']:.4f}")
