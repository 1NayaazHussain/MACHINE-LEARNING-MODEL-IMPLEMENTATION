import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import roc_curve, confusion_matrix
import pandas as pd

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("ðŸ“Š Creating visualizations for the spam detection model...")

# Sample data for visualization (this would come from the main script)
model_names = ['Naive Bayes', 'Logistic Regression', 'SVM', 'Random Forest']
accuracies = [0.92, 0.94, 0.91, 0.93]
roc_aucs = [0.95, 0.96, 0.93, 0.94]

# Create figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Spam Detection Model Analysis', fontsize=16, fontweight='bold')

# 1. Model Comparison - Accuracy
axes[0, 0].bar(model_names, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
axes[0, 0].set_title('Model Accuracy Comparison')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_ylim(0.85, 1.0)
for i, v in enumerate(accuracies):
    axes[0, 0].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')

# 2. Model Comparison - ROC-AUC
axes[0, 1].bar(model_names, roc_aucs, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
axes[0, 1].set_title('Model ROC-AUC Comparison')
axes[0, 1].set_ylabel('ROC-AUC Score')
axes[0, 1].set_ylim(0.85, 1.0)
for i, v in enumerate(roc_aucs):
    axes[0, 1].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')

# 3. Sample Confusion Matrix
cm_sample = np.array([[15, 2], [1, 18]])
sns.heatmap(cm_sample, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'],
            ax=axes[1, 0])
axes[1, 0].set_title('Confusion Matrix (Sample)')
axes[1, 0].set_ylabel('True Label')
axes[1, 0].set_xlabel('Predicted Label')

# 4. Feature Importance (sample data)
features = ['free', 'win', 'call', 'prize', 'urgent', 'click', 'text', 'offer']
importance = [0.15, 0.12, 0.10, 0.09, 0.08, 0.07, 0.06, 0.05]

axes[1, 1].barh(features, importance, color='#FFA07A')
axes[1, 1].set_title('Top Features (Sample)')
axes[1, 1].set_xlabel('Importance')

plt.tight_layout()
plt.show()

# ROC Curve visualization
plt.figure(figsize=(10, 8))

# Sample ROC curve data
fpr_nb = np.array([0.0, 0.05, 0.1, 0.15, 1.0])
tpr_nb = np.array([0.0, 0.85, 0.92, 0.95, 1.0])

fpr_lr = np.array([0.0, 0.03, 0.08, 0.12, 1.0])
tpr_lr = np.array([0.0, 0.88, 0.94, 0.97, 1.0])

fpr_svm = np.array([0.0, 0.06, 0.12, 0.18, 1.0])
tpr_svm = np.array([0.0, 0.82, 0.90, 0.93, 1.0])

fpr_rf = np.array([0.0, 0.04, 0.09, 0.14, 1.0])
tpr_rf = np.array([0.0, 0.86, 0.93, 0.96, 1.0])

plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_aucs[0]:.3f})', linewidth=2)
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_aucs[1]:.3f})', linewidth=2)
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_aucs[2]:.3f})', linewidth=2)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_aucs[3]:.3f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - Model Comparison')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Dataset distribution
labels = ['Ham', 'Spam']
sizes = [20, 16]  # Sample sizes
colors = ['#66b3ff', '#ff9999']

plt.figure(figsize=(8, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Dataset Distribution')
plt.axis('equal')
plt.show()

print("âœ… Visualizations created successfully!")

